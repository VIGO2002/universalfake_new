import os
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import average_precision_score, accuracy_score
import pandas as pd
from tqdm import tqdm
from PIL import Image
import numpy as np
import random

# ================= âš™ï¸ é…ç½®åŒºåŸŸ (è¯·æ ¹æ®å®é™…æƒ…å†µå¾®è°ƒ) =================

# 1. çœŸå®å›¾ç‰‡åŸºå‡†è·¯å¾„ (å¿…é¡»è®¾ç½®æ­£ç¡®)
# å»ºè®®ä½¿ç”¨ CNNDetection çš„ val æ–‡ä»¶å¤¹ï¼Œæˆ–è€… train/0_real
REAL_ROOT = "/root/autodl-tmp/datasets/CNNDetection/val"

# 2. æƒé‡æ–‡ä»¶å¤¹è·¯å¾„
CHECKPOINT_DIR = "./checkpoints/effort_universal_repro"

# 3. è¦æµ‹è¯•çš„ Epoch åˆ—è¡¨ (å»ºè®®æµ‹ 1-9)
EPOCHS_TO_TEST = list(range(1, 10))

# 4. æ¯ä¸ªæ•°æ®é›†æœ€å¤§æµ‹è¯•æ•°é‡ (500ä»£è¡¨: 500çœŸ + 500å‡ = 1000å¼ )
# è°ƒå¤§è¿™ä¸ªæ•°å€¼ç»“æœæ›´å‡†ï¼Œä½†é€Ÿåº¦ä¼šå˜æ…¢
MAX_SAMPLES_PER_CLASS = 500

# 5. æ•°æ®é›†è·¯å¾„æ˜ å°„ (æ ¹æ®ä½ æä¾›çš„ ls ç»“æœé…ç½®)
FAKE_DATASETS = {
    # --- GAN å®¶æ— (CNNDetection) ---
    "ProGAN": "/root/autodl-tmp/datasets/CNNDetection/progan",
    "CycleGAN": "/root/autodl-tmp/datasets/CNNDetection/cyclegan",
    "BigGAN": "/root/autodl-tmp/datasets/CNNDetection/biggan",
    "StyleGAN": "/root/autodl-tmp/datasets/CNNDetection/stylegan",
    "StyleGAN2": "/root/autodl-tmp/datasets/CNNDetection/stylegan2",
    "GauGAN": "/root/autodl-tmp/datasets/CNNDetection/gaugan",
    "StarGAN": "/root/autodl-tmp/datasets/CNNDetection/stargan",
    "DeepFake": "/root/autodl-tmp/datasets/CNNDetection/deepfake",

    # --- Diffusion å®¶æ— (Diffusion) ---
    "LDM_200": "/root/autodl-tmp/datasets/Diffusion/ldm_200",
    "LDM_200_cfg": "/root/autodl-tmp/datasets/Diffusion/ldm_200_cfg",
    "LDM_100": "/root/autodl-tmp/datasets/Diffusion/ldm_100",
    "Glide_100_27": "/root/autodl-tmp/datasets/Diffusion/glide_100_27",
    "Glide_50_27": "/root/autodl-tmp/datasets/Diffusion/glide_50_27",
    "Glide_100_10": "/root/autodl-tmp/datasets/Diffusion/glide_100_10",
    "DALLE": "/root/autodl-tmp/datasets/Diffusion/dalle",
    
    # --- é«˜éš¾åº¦ ---
    "Guided": "/root/autodl-tmp/datasets/Diffusion/guided",
}

# ===================================================================

class BinaryEvalDataset(Dataset):
    def __init__(self, real_root, fake_root, transform=None, max_samples=500):
        self.transform = transform
        self.samples = []
        
        # --- 1. åŠ è½½çœŸå›¾ (Label 0) ---
        real_imgs = []
        if os.path.exists(real_root):
            for root, _, files in os.walk(real_root):
                # ğŸ›¡ï¸ è¿‡æ»¤ï¼šå¦‚æœåœ¨çœŸå›¾ç›®å½•é‡Œå‘ç°äº† 'fake' å­—æ ·çš„æ–‡ä»¶å¤¹ï¼Œè·³è¿‡
                if 'fake' in root.lower():
                    continue
                for file in files:
                    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp', '.webp')):
                        real_imgs.append((os.path.join(root, file), 0))
        
        # --- 2. åŠ è½½å‡å›¾ (Label 1) ---
        fake_imgs = []
        if os.path.exists(fake_root):
            for root, _, files in os.walk(fake_root):
                # ğŸ›¡ï¸ æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœåœ¨å‡å›¾ç›®å½•é‡Œå‘ç°äº† '0_real' æˆ– 'real'ï¼Œå¿…é¡»è·³è¿‡ï¼
                # ä¹‹å‰å°±æ˜¯è¿™é‡ŒæŠŠçœŸå›¾å½“å‡å›¾è¯»äº†ï¼Œå¯¼è‡´å‡†ç¡®ç‡åªæœ‰ 50%
                if '0_real' in root or 'real' in root.lower():
                    continue
                for file in files:
                    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp', '.webp')):
                        fake_imgs.append((os.path.join(root, file), 1))

        # --- 3. æ•°æ®é‡‡æ ·ä¸å¹³è¡¡ ---
        # æ‰“ä¹±é¡ºåº
        random.shuffle(real_imgs)
        random.shuffle(fake_imgs)
        
        # æˆªæ–­åˆ°æœ€å¤§æ•°é‡
        if max_samples:
            real_imgs = real_imgs[:max_samples]
            fake_imgs = fake_imgs[:max_samples]
        
        # å¼ºåˆ¶æ•°é‡å¹³è¡¡ (å–æœ€å°å€¼)ï¼Œç¡®ä¿çœŸå‡æ¯”ä¾‹ 1:1
        min_len = min(len(real_imgs), len(fake_imgs))
        
        if min_len == 0:
            print(f"âš ï¸  [Warning] Dataset empty or imbalanced! Real: {len(real_imgs)}, Fake: {len(fake_imgs)}")
            self.samples = []
        else:
            self.samples = real_imgs[:min_len] + fake_imgs[:min_len]
            print(f"    âœ… Loaded: {min_len} Real + {min_len} Fake = {len(self.samples)} Total")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        try:
            img = Image.open(path).convert('RGB')
            if self.transform:
                img = self.transform(img)
            return img, label
        except Exception as e:
            # é‡åˆ°åå›¾è¿”å›é»‘å›¾ï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­
            return torch.zeros(3, 224, 224), label

def load_model(epoch, device):
    """åŠ è½½æ¨¡å‹å¹¶å¤„ç†æƒé‡é”®åä¸åŒ¹é…é—®é¢˜"""
    from models.clip_models import ClipModel
    try:
        # åˆå§‹åŒ–æ¨¡å‹ (å‚æ•°å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´)
        model = ClipModel(
            name='openai/clip-vit-large-patch14', 
            num_classes=1, 
            fix_backbone=True, 
            use_svd=True, 
            svd_rank_ratio=0.25
        )
        
        ckpt_path = os.path.join(CHECKPOINT_DIR, f"model_epoch_{epoch}.pth")
        if not os.path.exists(ckpt_path):
            print(f"âŒ Checkpoint not found: {ckpt_path}")
            return None
            
        print(f"âš¡ï¸ Loading Epoch {epoch} weights...")
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(ckpt_path, map_location='cpu')
        
        # å…¼å®¹ä¸åŒçš„ä¿å­˜æ ¼å¼
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        # å»é™¤ 'module.' å‰ç¼€ (å¦‚æœæ˜¯ DataParallel ä¿å­˜çš„)
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('module.'):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
            
        model.load_state_dict(new_state_dict, strict=False)
        model.to(device)
        model.eval()
        return model
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None

def main():
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°
    random.seed(42)
    torch.manual_seed(42)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ Using device: {device}")
    
    # CLIP æ ‡å‡†é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])

    results_data = []

    # --- å¤–å±‚å¾ªç¯ï¼šéå†æ‰€æœ‰ Epoch ---
    for epoch in EPOCHS_TO_TEST:
        print(f"\n{'='*20} Testing Epoch {epoch} {'='*20}")
        model = load_model(epoch, device)
        if model is None: continue
        
        epoch_result = {'Epoch': epoch}
        
        # --- å†…å±‚å¾ªç¯ï¼šéå†æ‰€æœ‰æ•°æ®é›† ---
        for ds_name, fake_path in FAKE_DATASETS.items():
            print(f"ğŸ“‚ Dataset: {ds_name}")
            
            # åˆå§‹åŒ–æ•°æ®é›†
            dataset = BinaryEvalDataset(REAL_ROOT, fake_path, transform=transform, max_samples=MAX_SAMPLES_PER_CLASS)
            
            if len(dataset) == 0:
                epoch_result[ds_name] = 0.0
                continue
            
            loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)
            
            y_true = []
            y_scores = []
            
            with torch.no_grad():
                for imgs, labels in tqdm(loader, leave=False, desc=f"Evaluating {ds_name}"):
                    imgs = imgs.to(device)
                    # å‰å‘ä¼ æ’­
                    logits = model(imgs)
                    # è®¡ç®—æ¦‚ç‡: Softmax åå– Fake ç±» (index 1) çš„æ¦‚ç‡
                    probs = torch.softmax(logits, dim=1)[:, 1]
                    
                    y_true.extend(labels.cpu().numpy())
                    y_scores.extend(probs.cpu().numpy())
            
            # è®¡ç®—æŒ‡æ ‡
            ap = average_precision_score(y_true, y_scores) * 100
            acc = accuracy_score(y_true, [1 if p > 0.5 else 0 for p in y_scores]) * 100
            
            print(f"   ğŸ‘‰ AP: {ap:.2f}% | Acc: {acc:.2f}%")
            epoch_result[ds_name] = ap
        
        results_data.append(epoch_result)

    # --- ä¿å­˜ç»“æœ ---
    print("\n" + "="*50)
    print("ğŸ† FINAL BENCHMARK RESULTS (AP %)")
    print("="*50)
    
    df = pd.DataFrame(results_data)
    df = df.set_index('Epoch')
    print(df)
    
    csv_filename = "benchmark_final_results.csv"
    df.to_csv(csv_filename)
    print(f"\nğŸ’¾ Results saved to {csv_filename}")

if __name__ == "__main__":
    main()