import os
import torch
import torch.nn as nn
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import average_precision_score, accuracy_score
from tqdm import tqdm
from PIL import Image
import random
import numpy as np

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================

# 1. ä½ çš„æœ€å¼ºæ¨¡å‹ Epoch
EPOCH = 8
DEVICE = "cuda"

# 2. ä¹‹å‰æ ¡å‡†çš„é»„é‡‘é˜ˆå€¼ (ProGAN Source Domain)
FIXED_THRESHOLD = 0.0010

# 3. æœ¬åœ° CLIP æƒé‡è·¯å¾„ (ä½ åˆšæ‰ ls ç¡®è®¤è¿‡çš„è·¯å¾„)
CLIP_LOCAL_PATH = "/root/autodl-tmp/pretrained_models/clip-vit-large-patch14"

# 4. GenImage æ ¹ç›®å½•
GENIMAGE_ROOT = "/root/autodl-tmp/GenImage"

# 5. æ•°æ®é›†è·¯å¾„æ˜ å°„
DATASET_PATHS = {
    "Midjourney":       ("Midjourney",             "imagenet_midjourney"),
    "Stable Diffusion v1.4": ("stable_diffusion_v_1_4", "imagenet_ai_0419_sdv4"),
    "Stable Diffusion v1.5": ("stable_diffusion_v_1_5", "imagenet_ai_0424_sdv5"),
    "ADM":              ("ADM",                    "imagenet_ai_0508_adm"),
    "Glide":            ("glide",                  "imagenet_glide"),
    "Wukong":           ("wukong",                 "imagenet_ai_0424_wukong"),
    "VQDM":             ("VQDM",                   "imagenet_ai_0419_vqdm"),
}

# 6. æµ‹è¯•æ ·æœ¬æ•° (2000å¼ è¶³å¤Ÿæƒå¨ä¸”é€Ÿåº¦å¿«)
MAX_SAMPLES = 2000

# 7. ä½ çš„æƒé‡ä¿å­˜æ–‡ä»¶å¤¹ (æ ¹æ®ä¹‹å‰ benchmark_final.py çš„è®¾ç½®)
CHECKPOINT_DIR = "./checkpoints/effort_universal_repro"

# ===============================================

def load_model(epoch, device):
    """
    åŠ è½½æ¨¡å‹ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ° CLIP æƒé‡ï¼Œæ— éœ€è”ç½‘
    """
    from models.clip_models import ClipModel
    try:
        print(f"âš¡ï¸ Loading CLIP from LOCAL path: {CLIP_LOCAL_PATH}")
        
        # åˆå§‹åŒ–æ¨¡å‹ (æŒ‡å®šæœ¬åœ°è·¯å¾„)
        model = ClipModel(
            name=CLIP_LOCAL_PATH,  # <--- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨æœ¬åœ°è·¯å¾„
            num_classes=1, 
            fix_backbone=True, 
            use_svd=True, 
            svd_rank_ratio=0.25
        )
        
        ckpt_path = os.path.join(CHECKPOINT_DIR, f"model_epoch_{epoch}.pth")
        if not os.path.exists(ckpt_path):
            print(f"âŒ Checkpoint not found: {ckpt_path}")
            return None
            
        print(f"âš¡ï¸ Loading Epoch {epoch} weights from {ckpt_path}...")
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        checkpoint = torch.load(ckpt_path, map_location='cpu')
        
        # å…¼å®¹æƒé‡å­—å…¸å¤„ç†
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        # å»é™¤ 'module.' å‰ç¼€
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('module.'):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
            
        model.load_state_dict(new_state_dict, strict=False)
        model.to(device)
        model.eval()
        return model
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None

class GenImageDataset(Dataset):
    def __init__(self, root_dir, transform=None, max_samples=None):
        self.transform = transform
        self.samples = []
        
        # å®šä¹‰è·¯å¾„: nature=çœŸ(0), ai=å‡(1)
        real_dir = os.path.join(root_dir, 'nature')
        fake_dir = os.path.join(root_dir, 'ai')
        
        # 1. åŠ è½½çœŸå›¾ (nature)
        real_imgs = []
        if os.path.exists(real_dir):
            for root, _, files in os.walk(real_dir):
                for file in files:
                    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):
                        real_imgs.append((os.path.join(root, file), 0))
        
        # 2. åŠ è½½å‡å›¾ (ai)
        fake_imgs = []
        if os.path.exists(fake_dir):
            for root, _, files in os.walk(fake_dir):
                for file in files:
                    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):
                        fake_imgs.append((os.path.join(root, file), 1))
        
        # 3. å¹³è¡¡ä¸é‡‡æ ·
        random.shuffle(real_imgs)
        random.shuffle(fake_imgs)
        
        if max_samples:
            real_imgs = real_imgs[:max_samples]
            fake_imgs = fake_imgs[:max_samples]
            
        # å¼ºåˆ¶å¹³è¡¡ (å–æœ€å°å€¼)
        min_len = min(len(real_imgs), len(fake_imgs))
        self.samples = real_imgs[:min_len] + fake_imgs[:min_len]
        
        print(f"    Found: {len(real_imgs)} Real (nature), {len(fake_imgs)} Fake (ai)")
        print(f"    Loaded: {min_len} Real + {min_len} Fake = {len(self.samples)} Total")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        try:
            img = Image.open(path).convert('RGB')
            if self.transform:
                img = self.transform(img)
            return img, label
        except Exception as e:
            return torch.zeros(3, 224, 224), label

def main():
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    random.seed(42)
    
    # é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])

    model = load_model(EPOCH, DEVICE)
    if model is None:
        return

    print(f"\n{'='*60}")
    print(f"ğŸš€ GenImage Benchmark (Cross-Domain Generalization)")
    print(f"ğŸ¯ Threshold: {FIXED_THRESHOLD} (Calibrated on ProGAN)")
    print(f"{'='*60}\n")

    results = []

    for name, (folder, subfolder) in DATASET_PATHS.items():
        # æ‹¼æ¥å®Œæ•´è·¯å¾„ï¼š/root/.../GenImage/Midjourney/imagenet_midjourney/val
        val_path = os.path.join(GENIMAGE_ROOT, folder, subfolder, "val")
        
        print(f"ğŸ“‚ Testing: {name}")
        # print(f"   Path: {val_path}")
        
        if not os.path.exists(val_path):
            print(f"âŒ Path not found! Skipping... ({val_path})")
            continue
            
        dataset = GenImageDataset(val_path, transform=transform, max_samples=MAX_SAMPLES)
        
        if len(dataset) == 0:
            print("âš ï¸  Dataset empty. Skipping.")
            continue
            
        loader = DataLoader(dataset, batch_size=32, num_workers=4, pin_memory=True)
        
        y_true, y_scores = [], []
        
        with torch.no_grad():
            for imgs, labels in tqdm(loader, leave=False, desc=f"Evaluating {name}"):
                imgs = imgs.to(DEVICE)
                logits = model(imgs)
                probs = torch.softmax(logits, dim=1)[:, 1]
                
                y_true.extend(labels.cpu().numpy())
                y_scores.extend(probs.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        ap = average_precision_score(y_true, y_scores) * 100
        y_pred = [1 if s >= FIXED_THRESHOLD else 0 for s in y_scores]
        acc = accuracy_score(y_true, y_pred) * 100
        
        print(f"   ğŸ‘‰ Result: AP={ap:.2f}% | Acc={acc:.2f}%")
        results.append({"Dataset": name, "AP": ap, "Acc": acc})

    print("\n" + "="*60)
    print("ğŸ† FINAL GENIMAGE RESULTS (ProGAN Model)")
    print("="*60)
    print(f"{'Dataset':<25} | {'AP (%)':<10} | {'Acc (%)':<10}")
    print("-" * 55)
    for res in results:
        print(f"{res['Dataset']:<25} | {res['AP']:<10.2f} | {res['Acc']:<10.2f}")
    print("-" * 55)
    print("(Note: Zero-shot results using ProGAN-calibrated threshold)")

if __name__ == "__main__":
    main()