import os
import torch
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
from sklearn.metrics import average_precision_score, accuracy_score
from models.trainer import Trainer
from options.train_options import TrainOptions
import numpy as np
from tqdm import tqdm

def test_generalization(epoch_num, test_dataset_path):
    dataset_name = os.path.basename(test_dataset_path)
    print(f"\n{'='*20} Testing Epoch {epoch_num} on {dataset_name} {'='*20}")
    
    # --- 1. ä¼ªé€ å‚æ•° (Mock Options) ---
    # ã€å…³é”®ä¿®å¤ã€‘å¿…é¡»æ‰‹åŠ¨æŒ‡å®šå’Œè®­ç»ƒæ—¶ä¸€æ ·çš„å‚æ•°ï¼
    opt = TrainOptions().parse(print_options=False)
    opt.isTrain = False
    opt.gpu_ids = [0]
    opt.name = 'effort_universal_repro'
    opt.checkpoints_dir = './checkpoints'
    
    # [æ ¸å¿ƒä¿®å¤ç‚¹] å¼ºåˆ¶æŒ‡å®šæ¨¡å‹æ¶æ„
    opt.arch = 'CLIP:ViT-L/14_svd' 
    opt.fix_backbone = True
    opt.noise_std = 0.02
    
    # --- 2. åˆå§‹åŒ–æ¨¡å‹ ---
    try:
        model = Trainer(opt)
        model.eval()
    except Exception as e:
        print(f"âŒ Model Init Error: {e}")
        return
    
    # --- 3. æ‰‹åŠ¨åŠ è½½æƒé‡ ---
    ckpt_path = os.path.join(opt.checkpoints_dir, opt.name, f'model_epoch_{epoch_num}.pth')
    print(f"âš¡ï¸ Loading weights from: {ckpt_path}")
    
    if not os.path.exists(ckpt_path):
        print(f"âŒ Error: File not found: {ckpt_path}")
        return

    try:
        checkpoint = torch.load(ckpt_path, map_location='cpu') # å…ˆåŠ è½½åˆ°CPUé˜²çˆ†æ˜¾å­˜
        
        # è‡ªåŠ¨æ‹†åŒ…é€»è¾‘
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
            
        # åŠ è½½åˆ°æ¨¡å‹
        if hasattr(model.model, "module"):
            model.model.module.load_state_dict(state_dict)
        else:
            model.model.load_state_dict(state_dict)
        print("âœ… Weights loaded!")
    except Exception as e:
        print(f"âŒ Load Error: {e}")
        return
    
    # --- 4. å‡†å¤‡æ•°æ® (Standard Loader) ---
    val_transform = transforms.Compose([
        transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))
    ])
    
    try:
        dataset = datasets.ImageFolder(root=test_dataset_path, transform=val_transform)
        # BigGANå¯èƒ½æ²¡æœ‰ 0_real/1_fake ç»“æ„ï¼Œè¿™é‡Œè¦åšä¸ªå…¼å®¹æ€§æ£€æŸ¥
        # å¦‚æœæ˜¯æŒ‰ç±»åˆ«åˆ†çš„(bedroom, cat...)ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒä»¬éƒ½å½“åš Fake (å› ä¸ºè¿™æ˜¯BigGANç”Ÿæˆçš„)
        # ä½†ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å…ˆå‡è®¾ç›®å½•ç»“æ„æ˜¯æ ‡å‡†çš„ã€‚å¦‚æœæŠ¥é”™æˆ‘ä»¬å†æ”¹ã€‚
        dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)
        print(f"âœ… Indexed {len(dataset)} images.")
    except Exception as e:
        print(f"âŒ Data Error: {e}")
        return

    # --- 5. å¼€å§‹æ¨ç† ---
    y_true = []
    y_pred = []
    
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU
    model.model.cuda()
    
    with torch.no_grad():
        for i, data in tqdm(enumerate(dataloader), total=len(dataloader)):
            model.set_input(data)
            model.test()
            
            # è·å–é¢„æµ‹ç»“æœ
            pred = model.output
            
            # å– "Fake" ç±»çš„æ¦‚ç‡
            if pred.shape[1] == 1:
                prob = torch.sigmoid(pred).cpu().numpy().flatten()
            else:
                prob = torch.softmax(pred, dim=1)[:, 1].cpu().numpy()
            
            label = data[1].cpu().numpy()
            
            y_true.extend(label)
            y_pred.extend(prob)

    # --- 6. è®¡ç®—æŒ‡æ ‡ ---
    if len(np.unique(y_true)) < 2:
        print("âš ï¸ Warning: Only one class detected in test set. mAP might be undefined.")
        # å¦‚æœåªæœ‰ä¸€ç±»ï¼Œæˆ‘ä»¬åªæ‰“å°å¹³å‡é¢„æµ‹åˆ†
        print(f"   Avg Prediction Score: {np.mean(y_pred):.4f}")
    else:
        mAP = average_precision_score(y_true, y_pred)
        y_pred_binary = [1 if p > 0.5 else 0 for p in y_pred]
        acc = accuracy_score(y_true, y_pred_binary)
        
        print(f"\nğŸ† Result for Epoch {epoch_num} on {dataset_name}:")
        print(f"   mAP: {mAP:.4f}")
        print(f"   Acc: {acc:.4f}")
    print("="*60)

if __name__ == "__main__":
    # è¯·ç¡®è®¤è¿™ä¸ªè·¯å¾„ä¸‹ç¡®å®æœ‰å›¾ç‰‡ï¼Œä¸”æœ€å¥½åŒ…å« 0_real å’Œ 1_fake
    # å¦‚æœ BigGAN åªæœ‰å‡å›¾ï¼ŒmAP æ— æ³•è®¡ç®—ï¼Œåªèƒ½çœ‹é¢„æµ‹æ¦‚ç‡
    # ä¸ºäº†ä¸¥è°¨ï¼Œå»ºè®®æµ‹è¯•åŒ…å«çœŸå‡å›¾çš„æ•°æ®é›†

    # è‡ªåŠ¨æœç´¢ checkpoints ç›®å½•ä¸‹çš„æ‰€æœ‰ epoch
    print(f"\nğŸš€ Scanning all checkpoints in ./checkpoints/effort_universal_repro ...")
    
    # æ‰¾åˆ°æ‰€æœ‰çš„ model_epoch_X.pth æ–‡ä»¶
    ckpt_dir = os.path.join('./checkpoints', 'effort_universal_repro')
    files = os.listdir(ckpt_dir)
    epochs = []
    for f in files:
        if f.startswith('model_epoch_') and f.endswith('.pth') and 'init' not in f:
            # æå–æ•°å­—
            try:
                ep = int(f.split('_')[-1].split('.')[0])
                epochs.append(ep)
            except:
                pass
    
    # æŒ‰ä»å°åˆ°å¤§æ’åº
    epochs.sort()
    print(f"ğŸ“‹ Found epochs: {epochs}")

    TEST_PATH = "/root/autodl-tmp/datasets/CNNDetection/biggan" 
    
    # å¾ªç¯æµ‹è¯•æ‰€æœ‰ Epoch
    results = {}
    for ep in epochs:
        # ä¸ºäº†é˜²æ­¢å†…å­˜æ³„æ¼ï¼Œè¿™é‡Œåªåšç®€å•çš„è°ƒç”¨ã€‚
        # å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå¯èƒ½éœ€è¦æŠŠ test_generalization é‡Œçš„ model åˆå§‹åŒ–ç§»åˆ°å¤–é¢ã€‚
        # ä½†è€ƒè™‘åˆ°åªæ˜¯æµ‹è¯•ï¼Œåº”è¯¥é—®é¢˜ä¸å¤§ã€‚
        mAP, acc = test_generalization(ep, TEST_PATH)
        results[ep] = mAP

    # æ‰“å°æœ€ç»ˆæ’å
    print(f"\n{'='*20} Final Ranking (mAP) {'='*20}")
    sorted_res = sorted(results.items(), key=lambda x: x[1], reverse=True)
    for ep, score in sorted_res:
        print(f"Epoch {ep}: mAP {score:.4f} {'ğŸ¥‡' if score == sorted_res[0][1] else ''}")
